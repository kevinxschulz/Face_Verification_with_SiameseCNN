{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fe315b",
   "metadata": {
    "id": "47fe315b"
   },
   "source": [
    "# Siamese CNN for Triplet Loss and Contrastive Loss: Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef01dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTRODUCTION\n",
    "#  A Siamese CNN is a convolutional neural network which produces embeddings of several images, which enables to compare the embeddings afterwards and determine how similar they are. \n",
    "# We have implemented this in two ways: With the triplet loss and the contrastive loss. Both of these loss functions can be used to train the model within this notebook.\n",
    "# The two loss functions require a different dataloader and predict the treshold for similarity in different ways. Since BOTH LOSS FUNCTIONS PERFORMED SIMILARLY WELL, \n",
    "# we decided to include them both to show how differently a Siamese CNN can be implemented. \n",
    "\n",
    "# TRIPLET LOSS: EXPLANATION\n",
    "# Our implementation of TripletLoss is based on the paper \"FaceNet: A Unified Embedding for Face Recognition and Clustering\" (Schroff et al., 2015, http://www.arxiv.org/pdf/1503.03832). \n",
    "# TripletLoss is a loss function which encourages similar embeddings to have small distances, while it encourages different embeddings to have large distances. During training it therefore\n",
    "# needs a triplet of embeddings: An anchor, a positive and a negative. \n",
    "# To improve training performance, only triplets which are difficult to the model are chosen. This is done by letting the model compute large batches of images and produce embeddings,\n",
    "# and only then forming difficult triplets. These can then be passed to the loss function and used to train the model effectively. \n",
    "\n",
    "# CONTRASTIVE LOSS: EXPLANATION\n",
    "# The Constrative Loss function also produces embeddings and tries to maximise the distance for image pairs of different persons, while minimising the distance for image pairs of the same person. \n",
    "# To feed two images to the model, a custom Dataset is implemented which randomly chooses them from the whole data.\n",
    "\n",
    "# TLDR: We have implemented two loss functions, and one can choose which one to use within the \"main\" cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d130aeb",
   "metadata": {
    "id": "3d130aeb"
   },
   "source": [
    "## Step 1: Preparation of Google Collab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e75d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DATA\n",
    "# For the Triplet Loss a data split of Training(90%), Validation(5%) and Testing (5%) was used. \n",
    "# For the Contrastive Loss a data split of Training(80%), Validation(10%) and Testing (20%) was used.\n",
    "# Both models tested and validated their performance on an unseen fraction of the provided synthetic data. \n",
    "# Because the ImageFolder Dataset provides the most utility, the data was split beforehand in the filesystem. \n",
    "# When trying to use randomSplit or train_test_split, there were difficulties with further handling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BuXftw87v49a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BuXftw87v49a",
    "outputId": "ba78d794-761c-4ade-dfa9-7e1154275255"
   },
   "outputs": [],
   "source": [
    "# Connect Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33901e79",
   "metadata": {
    "id": "33901e79"
   },
   "outputs": [],
   "source": [
    "# Create a folder structure for the training, validation and testing data\n",
    "!mkdir /content/datasets\n",
    "!mkdir /content/datasets/train_zip\n",
    "!mkdir /content/datasets/val_zip\n",
    "!mkdir /content/datasets/test_zip\n",
    "!mkdir /content/datasets/train_img\n",
    "!mkdir /content/datasets/val_img\n",
    "!mkdir /content/datasets/test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b4c624",
   "metadata": {
    "id": "47b4c624"
   },
   "outputs": [],
   "source": [
    "# Copy the corresponding training, validation and testing data into the previously created folders, which lay in the runningtime environment of google colab\n",
    "# Duration: approx 2 minutes\n",
    "!cp /content/drive/MyDrive/Colab_Notebooks/da2_train_aug.zip /content/datasets/train_zip\n",
    "!cp /content/drive/MyDrive/Colab_Notebooks/da2_val.zip /content/datasets/val_zip\n",
    "!cp /content/drive/MyDrive/Colab_Notebooks/da2_test.zip /content/datasets/test_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6de33d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d6de33d",
    "outputId": "df793d42-a01e-421b-a738-3dc851829997"
   },
   "outputs": [],
   "source": [
    "# Unzip all files - Duration: Approx. 3 minutes\n",
    "!unzip /content/datasets/train_zip/da2_train.zip -d /content/datasets/train_img\n",
    "!unzip /content/datasets/val_zip/da2_val.zip -d /content/datasets/val_img\n",
    "!unzip /content/datasets/test_zip/da2_test.zip -d /content/datasets/test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b797c8",
   "metadata": {
    "id": "c6b797c8"
   },
   "source": [
    "## Step 2: Define the actual program elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9bb3f3",
   "metadata": {
    "id": "1d9bb3f3"
   },
   "source": [
    "### Step 2.1: Specify used imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U_bBrNH_v-u0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_bBrNH_v-u0",
    "outputId": "797c4d27-bccb-4ebd-efb9-09a4622d2ad8"
   },
   "outputs": [],
   "source": [
    "# INSTALL\n",
    "# If not already installed. This library is used to implement triplet loss and online triplet mining.\n",
    "!pip install pytorch_metric_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9194fa4c",
   "metadata": {
    "id": "9194fa4c"
   },
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import RandomApply\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers, samplers\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import torchvision\n",
    "import itertools\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db6d7e",
   "metadata": {
    "id": "e2db6d7e"
   },
   "source": [
    "### Step 2.2: Define the DataLoader for the Contrastive Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a172ec",
   "metadata": {
    "id": "09a172ec"
   },
   "outputs": [],
   "source": [
    "# DATALOADER CONTRASTIVE LOSS\n",
    "# This is the dataloader used for the contrastive loss implementation. Two images are returned,\n",
    "# and with a 50/50 probability they belong to the same person or not. \n",
    "\n",
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset, transform=None):\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Select a random image from the given image folder\n",
    "        img0_tuple = random.choice(self.image_folder_dataset.imgs)\n",
    "\n",
    "        # Select if you choose two images of the same class (similar person) or different class (different person)\n",
    "        should_get_same_class = random.randint(0, 1)\n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                # Keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.image_folder_dataset.imgs)\n",
    "                if img0_tuple[1] == img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                # Keep looping till a different class image is found\n",
    "                img1_tuple = random.choice(self.image_folder_dataset.imgs)\n",
    "                if img0_tuple[1] != img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        # Open the image\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "\n",
    "        # Extract the image path\n",
    "        img0_path = img0_tuple[0]\n",
    "        img1_path = img1_tuple[0]\n",
    "\n",
    "        # Label: 0 for same class, 1 for different class - Necessary for the contrastive loss function, we want to represent a distance,\n",
    "        # that represents if two images are similar (small distance) or different (large distance)\n",
    "        label = torch.tensor([int(img0_tuple[1] != img1_tuple[1])], dtype=torch.float32)\n",
    "\n",
    "        # Transform images to a tensor\n",
    "        img0 = self.transform(img0)\n",
    "        img1 = self.transform(img1)\n",
    "\n",
    "        # Return tuple, which contains the two choosen images with their corresponding paths and label\n",
    "        return img0, img1, img0_path, img1_path, label\n",
    "\n",
    "    # Check how many images the given image folder contains\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder_dataset.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc194b5",
   "metadata": {
    "id": "bcc194b5"
   },
   "source": [
    "### Step 2.3 Define the CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a63d573b",
   "metadata": {
    "id": "a63d573b"
   },
   "outputs": [],
   "source": [
    "# MODEL ARCHITECTURE\n",
    "# Used architecture with ~109.000 Parameters - It contains three convolutional layers, with a corresponding pooling layer,\n",
    "# a global average pooling and one fully connected layer\n",
    "class FaceRecognitionModel(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super(FaceRecognitionModel, self).__init__()\n",
    "\n",
    "        # Start with 3 input channels (RGB image), 32 output channels to extract initial features\n",
    "        # Kernel size 3 and padding/stride of 1 to retain the image size without elevating the parameter count (as would happen with a kernel size of 5)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Pooling to reduce spatial dimensions and speed up computation\n",
    "        # Also enhances translational invariance by reducing sensitivity to exact image positions\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Increasing depth with the second convolutional layer\n",
    "        # Still retaining the same size with a kernel size of 3 and padding/stride of 1, followed by pooling\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Third convolutional layer for further depth and feature extraction, followed by pooling again\n",
    "        # Not increasing the feature maps beyond 128 to keep the parameter count low\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Global Average Pooling reduces the spatial dimensions of the feature maps to a single value\n",
    "        # This significantly reduces the number of parameters and computation time\n",
    "        # It also helps in reducing overfitting and sensitivity to exact image positions\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layer to convert pooled features into a compact embedding vector\n",
    "        # Embedding represents high-dimensional data in a lower-dimensional space (128 dimensions in this case)\n",
    "        self.fc1 = nn.Linear(128, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)   # Flatten\n",
    "        x = self.fc1(x)             # Output: embedding vector\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c604ffb",
   "metadata": {
    "id": "2c604ffb"
   },
   "source": [
    "### Step 2.4: Define contrastive loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7806fb6",
   "metadata": {
    "id": "c7806fb6"
   },
   "outputs": [],
   "source": [
    "# CONTRASTIVE LOSS FUNCTION\n",
    "# Implementation of the contrastive loss function with default margin = 1.0\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "      # Calculate the euclidean distance and calculate the contrastive loss\n",
    "      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "\n",
    "      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "      return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a0b17",
   "metadata": {
    "id": "e99a0b17"
   },
   "source": [
    "### Step 2.5: Define functions for the training and validation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc0b825",
   "metadata": {
    "id": "3fc0b825"
   },
   "outputs": [],
   "source": [
    "# CONTRASTIVE LOSS TRAINING\n",
    "# Method to conduct a training with the given hyperparameters and the contrastive loss\n",
    "def train_contrastive(model, epoch_number, train_dataloader, val_dataloader, optimizer, criterion_name, device, treshold, path_to_store_model_weights, path_to_store_model):\n",
    "    print(\"Start Training\")\n",
    "    loss_history = []\n",
    "    batch_iteration_counter = 0\n",
    "    validation_history = []\n",
    "    error_history = []\n",
    "\n",
    "\n",
    "    for epoch in range(epoch_number):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch_idx, (img0, img1, img0_path, img1_path, labels) in enumerate(train_dataloader):\n",
    "            img0, img1, labels = img0.to(device), img1.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output1 = model(img0)\n",
    "            output2 = model(img1)\n",
    "\n",
    "            criterion = ContrastiveLoss(margin=1.0)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            total_train += labels.size(0)\n",
    "            loss_history.append(total_train_loss / (batch_idx + 1))\n",
    "            batch_iteration_counter += 1\n",
    "\n",
    "            # Log and save the loss every 50 batches\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                current_loss = total_train_loss / (batch_idx + 1)\n",
    "                print(f\"Batch {batch_idx+1}/{len(train_dataloader)}, Current Loss: {current_loss:.4f}\")\n",
    "\n",
    "        train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epoch_number}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        try:\n",
    "            torch.save(model, f'{path_to_store_model_weights}/siamese_model_epoch_{epoch+1}.pth')\n",
    "            torch.save(model.state_dict(), f'{path_to_store_model}/siamese_model_weights_epoch{epoch+1}.pth')\n",
    "        except Exception as e:\n",
    "          print(f\"An error occured while storing the model or the model weights: {e}\")\n",
    "\n",
    "        validation_process, error_process = validate_contrastive(model, val_dataloader, device, treshold)\n",
    "        validation_history.append(validation_process)\n",
    "        error_history.append(error_process)\n",
    "    return loss_history, batch_iteration_counter, validation_history, validation_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3875d642",
   "metadata": {
    "id": "3875d642"
   },
   "outputs": [],
   "source": [
    "# CONTRASTIVE LOSS VALIDATION\n",
    "# Method to validate the model during the training process\n",
    "def validate_contrastive(model, val_dataloader, device, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    accuracy_process = []\n",
    "    error_process = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (img1, img2, img0_path, img1_path, labels) in enumerate(val_dataloader):\n",
    "          labels = labels.view(-1, 1).float()\n",
    "          img1 = img1.to(device)\n",
    "          img2 = img2.to(device)\n",
    "          labels = labels.to(device)\n",
    "          img1 = Variable(img1)\n",
    "          img2 = Variable(img2)\n",
    "          labels = Variable(labels)\n",
    "\n",
    "          output1 = model(img1)\n",
    "          output2 = model(img2)\n",
    "\n",
    "          # Calculate Similarity with euclidean distance\n",
    "          euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "          predicted = torch.tensor([0 if sd < threshold else 1 for sd in euclidean_distance]).to(device)\n",
    "\n",
    "          # # Also possible: Calculate similarity with Cosine Similarity\n",
    "          #cosine_similarity = F.cosine_similarity(output1, output2)\n",
    "          #predicted = torch.tensor([0 if tresh > threshold else 1 for tresh in cosine_similarity]).to(device)\n",
    "\n",
    "          predicted_array = predicted.cpu().numpy()\n",
    "          labels_array = labels.cpu().numpy().flatten().astype(int)\n",
    "\n",
    "          zero_matches = np.sum((labels_array == 0) & (predicted_array == 0))\n",
    "          one_matches = np.sum((labels_array == 1) & (predicted_array == 1))\n",
    "\n",
    "          total += labels.size(0)\n",
    "          correct += zero_matches + one_matches\n",
    "    accuracy = 100 * (correct / total)\n",
    "    error = 1 - (correct / total)\n",
    "\n",
    "    accuracy_process.append(accuracy)\n",
    "    error_process.append(error_process)\n",
    "    print('Validation Accuracy of the network: {:.2f}%'.format(accuracy))\n",
    "\n",
    "    return accuracy_process, error_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1d34fe",
   "metadata": {
    "id": "3f1d34fe"
   },
   "outputs": [],
   "source": [
    "# TRIPPLET LOSS TRAINING\n",
    "def train_tripplet(model, train_loader, val_loader, optimizer, mining_func, loss_fn, epoch_number, threshold, device, path_to_store_model_weights, path_to_store_model):\n",
    "    for epoch in range(epoch_number):\n",
    "      # TRAINING\n",
    "      print(\"Training\")\n",
    "      model.train()\n",
    "      for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # send all images through the model to produce embeddings\n",
    "        embeddings = model(data)\n",
    "\n",
    "        # call mining function to find difficult triplets\n",
    "        triplets = mining_func(embeddings, labels)\n",
    "\n",
    "        # call loss function, pass in difficult triplets\n",
    "        loss = loss_fn(embeddings, labels, triplets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Iteration {batch_idx}: Loss = {loss}, Number of mined triplets = {mining_func.num_triplets}\")\n",
    "\n",
    "      try:\n",
    "        torch.save(model, f'{path_to_store_model_weights}/siamese_model_epoch_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(), f'{path_to_store_model}/siamese_model_weights_epoch{epoch+1}.pth')\n",
    "      except Exception as e:\n",
    "          print(f\"An error occured while storing the model or the model weights: {e}\")\n",
    "\n",
    "      validate_tripplet(model, val_loader, device, threshold, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8187670",
   "metadata": {
    "id": "e8187670"
   },
   "outputs": [],
   "source": [
    "# TRIPPLET LOSS VALIDATION\n",
    "def validate_tripplet(model, val_loader, device, threshold, loss_fn):\n",
    "    # VALIDATION\n",
    "    print(\"Validation\")\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    num_batches = 0\n",
    "    val_loss_sum = 0\n",
    "    val_acc_sum = 0\n",
    "    num_pairs = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                num_batches += 1\n",
    "\n",
    "                # Reminder: 40 images from two identities are loaded in one batch.\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                embeddings = model(data)\n",
    "\n",
    "                # VALIDATION LOSS\n",
    "                # Calculate the loss based on all embeddings in the batch (no mining is used, therefore the loss function\n",
    "                # forms tripplets itself, see documentation of Pytorch metric Learning)\n",
    "                batch_loss = loss_fn(embeddings, labels)\n",
    "\n",
    "                # VALIDATION PREDICTIONS AND VALIDATION ACCURACY\n",
    "                # Create a tensor which indexes all possible pairs of images in the batch\n",
    "                idx = list(range(len(labels)))\n",
    "                pairs = list(itertools.combinations(idx, 2))\n",
    "                pairs_tensor = torch.tensor(pairs)\n",
    "\n",
    "                batch_predictions_made = 0\n",
    "                batch_correct = 0\n",
    "                for pair in pairs_tensor:\n",
    "                    num_pairs += 1\n",
    "\n",
    "                    label1 = labels[pair[0]].unsqueeze(0)\n",
    "                    label2 = labels[pair[1]].unsqueeze(0)\n",
    "                    label = torch.tensor(0 if label1 == label2 else 1).to(device)\n",
    "\n",
    "                    embedding1 = embeddings[pair[0]].unsqueeze(0)\n",
    "                    embedding2 = embeddings[pair[1]].unsqueeze(0)\n",
    "\n",
    "                    # CHOICE OF THRESHOLD - EUCLIDEAN OR COSINE DISTANCE?\n",
    "                    # In the FaceNet Paper, a squared euclidean distance treshold was used to determine wether\n",
    "                    # images are similar or not.\n",
    "\n",
    "                    # Instead you can also use the cosine_similarity, as it can be converted more intuitively\n",
    "                    # into the range of 0 to 1 because higher values represent higher similarity, which is\n",
    "                    # also requested by the case study.\n",
    "\n",
    "                    # SQUARED EUCLIDEAN DISTANCE TRESHOLD (not used)\n",
    "                    #output1N = F.normalize(embedding1, p=2, dim=1)\n",
    "                    #output2N = F.normalize(embedding2, p=2, dim=1)\n",
    "                    #squared_euclidean_distance = F.pairwise_distance(output1N, output2N)**2\n",
    "                    #prediction = torch.tensor([0 if tresh < 1.1 else 1 for tresh in squared_euclidean_distance]).to(device)\n",
    "\n",
    "                    # COSINE SIMILARITY TRESHOLD\n",
    "                    cosine_similarity = F.cosine_similarity(embedding1, embedding2)\n",
    "                    prediction = torch.tensor([0 if tresh > threshold else 1 for tresh in cosine_similarity]).to(device)\n",
    "\n",
    "\n",
    "                    batch_predictions_made += 1\n",
    "                    batch_correct += (prediction == label).sum().item()\n",
    "                    batch_accuracy = 100 * (batch_correct / batch_predictions_made)\n",
    "\n",
    "\n",
    "                val_loss_sum += batch_loss\n",
    "                val_loss = val_loss_sum / num_batches\n",
    "\n",
    "                val_acc_sum += batch_accuracy\n",
    "                val_acc = val_acc_sum / num_batches\n",
    "\n",
    "                print(\"Total Loss.: \",val_loss.item())\n",
    "                print(\"Total Acc.: \",val_acc)\n",
    "                print(\"Image Pairs Computed: \", num_pairs)\n",
    "                print(\"Batches Remaining: \", len(val_loader)-num_batches+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b2614",
   "metadata": {
    "id": "c30b2614"
   },
   "source": [
    "### Step 2.6 Define functions for making the actual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "072bb851",
   "metadata": {
    "id": "072bb851"
   },
   "outputs": [],
   "source": [
    "# CONTRASTIVE LOSS PREDICTION\n",
    "# Method to test the trained model - in contrast to the validation loop this method returns an evaluation.csv file which containts the image paths, the true label, the predicted label and the distance between two embeddings.\n",
    "def predict_contrastive(model, val_dataloader, device, threshold):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    distance = []\n",
    "    img1_paths = []\n",
    "    img2_paths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (img1, img2, img1_path, img2_path, labels) in enumerate(val_dataloader):\n",
    "          labels = labels.view(-1, 1).float()\n",
    "          img1 = img1.to(device)\n",
    "          img2 = img2.to(device)\n",
    "          labels = labels.to(device)\n",
    "          img1 = Variable(img1)\n",
    "          img2 = Variable(img2)\n",
    "          labels = Variable(labels)\n",
    "\n",
    "          output1 = model(img1)\n",
    "          output2 = model(img2)\n",
    "          euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "          predicted = torch.tensor([0 if sd < threshold else 1 for sd in euclidean_distance]).to(device)\n",
    "\n",
    "          predicted_array = predicted.cpu().numpy()\n",
    "          labels_array = labels.cpu().numpy().flatten().astype(int)\n",
    "          euclidean_distance = euclidean_distance.cpu().numpy().astype(float)\n",
    "\n",
    "          zero_matches = np.sum((labels_array == 0) & (predicted_array == 0))\n",
    "          one_matches = np.sum((labels_array == 1) & (predicted_array == 1))\n",
    "\n",
    "          true_labels.extend(labels_array)\n",
    "          predicted_labels.extend(predicted_array)\n",
    "          distance.extend(euclidean_distance)\n",
    "          img1_paths.extend(img1_path)\n",
    "          img2_paths.extend(img2_path)\n",
    "\n",
    "          total += labels.size(0)\n",
    "          correct += zero_matches + one_matches\n",
    "    accuracy = 100 * (correct / total)\n",
    "    print('Accuracy of the network on the test set: {:.2f}%'.format(accuracy))\n",
    "\n",
    "    evaluation_df = pd.DataFrame({\n",
    "    'Image1 Path': img1_paths,\n",
    "    'Image2 Path': img2_paths,\n",
    "    'True Label': true_labels,\n",
    "    'Predicted Label': predicted_labels,\n",
    "    'Distance': distance })\n",
    "\n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b39e2da6",
   "metadata": {
    "id": "b39e2da6"
   },
   "outputs": [],
   "source": [
    "# TRIPPLET LOSS PREDICTION\n",
    "def prediction_tripplet(model, test_loader, device, threshold, loss_fn):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    distance = []\n",
    "\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    num_batches = 0\n",
    "    test_loss_sum = 0\n",
    "    test_acc_sum = 0\n",
    "\n",
    "    num_pairs = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            num_batches += 1\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            embeddings = model(data)\n",
    "\n",
    "            #calculate the loss based on all embeddings in the batch\n",
    "            batch_loss = loss_fn(embeddings, labels)\n",
    "            #print(\"Batch Loss: \",batch_loss.item())\n",
    "\n",
    "            #create a tensor which indexes all possible pairs of images in the batch\n",
    "            idx = list(range(len(labels)))\n",
    "            pairs = list(itertools.combinations(idx, 2))\n",
    "            pairs_tensor = torch.tensor(pairs)\n",
    "\n",
    "            #for each pair, make the prediction and calculate accuracy\n",
    "            batch_predictions_made = 0\n",
    "            batch_correct = 0\n",
    "            for pair in pairs_tensor:\n",
    "                num_pairs += 1\n",
    "\n",
    "                label1 = labels[pair[0]].unsqueeze(0)\n",
    "                label2 = labels[pair[1]].unsqueeze(0)\n",
    "                label = torch.tensor(0 if label1 == label2 else 1).to(device)\n",
    "\n",
    "                embedding1 = embeddings[pair[0]].unsqueeze(0)\n",
    "                embedding2 = embeddings[pair[1]].unsqueeze(0)\n",
    "\n",
    "                #output1N = F.normalize(embedding1, p=2, dim=1)\n",
    "                #output2N = F.normalize(embedding2, p=2, dim=1)\n",
    "                #squared_euclidean_distance = F.pairwise_distance(output1N, output2N)**2\n",
    "                #prediction = torch.tensor([0 if thresh < 1.1 else 1 for tresh in squared_euclidean_distance]).to(device)\n",
    "\n",
    "                cosine_similarity = F.cosine_similarity(embedding1, embedding2)\n",
    "                prediction = torch.tensor([0 if tresh > threshold else 1 for tresh in cosine_similarity]).to(device)\n",
    "\n",
    "                true_labels.append(label.item())\n",
    "                predicted_labels.append(prediction.item())\n",
    "                distance.append(cosine_similarity.item())\n",
    "\n",
    "                #print(\"distance:\",cosine_similarity)\n",
    "                #print(\"prediction\", prediction.item(), \"label\", label.item())\n",
    "                #imshow(data[pair[0]], data[pair[1]])\n",
    "\n",
    "                batch_predictions_made += 1\n",
    "                batch_correct += (prediction == label).sum().item()\n",
    "                batch_accuracy = 100 * (batch_correct / batch_predictions_made)\n",
    "\n",
    "            #print(\"Batch Acc.: \",batch_accuracy)\n",
    "\n",
    "            test_loss_sum += batch_loss\n",
    "            test_loss = test_loss_sum / num_batches\n",
    "\n",
    "            test_acc_sum += batch_accuracy\n",
    "            test_acc = test_acc_sum / num_batches\n",
    "\n",
    "\n",
    "            print(\"Testing Loss.: \",test_loss.item())\n",
    "            print(\"Testing Accuracy.: \",test_acc)\n",
    "            print(\"Image Pairs Computed: \", num_pairs)\n",
    "            print(\"Batches Remaining: \", len(test_loader)-num_batches+1)\n",
    "\n",
    "    evaluation_df = pd.DataFrame({\n",
    "        'True Label': true_labels,\n",
    "        'Predicted Label': predicted_labels,\n",
    "        'Distance': distance })\n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970da02",
   "metadata": {
    "id": "8970da02"
   },
   "source": [
    "## Step 3: Define the program procedure for the corresponding loss function implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d416311d",
   "metadata": {
    "id": "d416311d"
   },
   "outputs": [],
   "source": [
    "# CONTRASTIVE LOSS PROCEDURE\n",
    "def run_contrastive(path_to_train_images, path_to_validation_images, path_to_test_images, siamese_model,\n",
    "                         epoch_number, batch_size_training, batch_size_validation, learning_rate, threshold,\n",
    "                         optimizer, device, path_to_store_model_weights, path_to_store_model):\n",
    "\n",
    "    # Step 1: Load the training, validation and test dataset with ImageFolder\n",
    "    train_dataset = datasets.ImageFolder(path_to_train_images)\n",
    "    val_dataset = datasets.ImageFolder(path_to_validation_images)\n",
    "    test_dataset = datasets.ImageFolder(path_to_test_images)\n",
    "\n",
    "    # Step 2: Transform to tensors\n",
    "    transformation = transforms.ToTensor()\n",
    "\n",
    "    # Step 3: Initialize the Network Data Loader for training and validation\n",
    "    siamese_train_dataset = SiameseNetworkDataset(train_dataset, transform=transformation)\n",
    "    siamese_val_dataset = SiameseNetworkDataset(val_dataset, transform=transformation)\n",
    "    siamese_test_dataset = SiameseNetworkDataset(test_dataset, transform=transformation)\n",
    "\n",
    "    # Step 4: Print out the number of trainable params\n",
    "    total_params = sum(p.numel() for p in siamese_model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of trainable parameters {total_params}\")\n",
    "\n",
    "    # Step 5: Load training-, validation- and testdataset with DataLoader\n",
    "    train_dataloader = DataLoader(siamese_train_dataset,\n",
    "                                  shuffle=True,\n",
    "                                  batch_size=batch_size_training)\n",
    "\n",
    "    val_dataloader = DataLoader(siamese_val_dataset,\n",
    "                                 shuffle=False,\n",
    "                                 batch_size=batch_size_validation)\n",
    "\n",
    "    test_dataloader = DataLoader(siamese_test_dataset,\n",
    "                                 shuffle=False,\n",
    "                                 batch_size=batch_size_validation)\n",
    "    # Step 6: Start Training Loop\n",
    "    loss_process, batch_iteration_counter, accuracy_process, error_process = train_contrastive(siamese_model, epoch_number, train_dataloader, val_dataloader, optimizer, \"Contrastive\", device, threshold, path_to_store_model_weights, path_to_store_model)\n",
    "\n",
    "    # Step 7: Make actual predictions\n",
    "    evaluation_df = predict_contrastive(siamese_model, test_dataloader, device, threshold)\n",
    "\n",
    "    # Step 8: Store evaluation_df in a csv file\n",
    "    evaluation_df.to_csv(\"contrastive_evaluation_df\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9164720",
   "metadata": {
    "id": "c9164720"
   },
   "outputs": [],
   "source": [
    "# TRIPPLET LOSS PROCEDURE\n",
    "def run_tripplet(path_to_training_images, path_to_validation_images, path_to_test_images, siamese_model,\n",
    "                         epoch_number, batch_size_training, batch_size_validation, learning_rate, threshold,\n",
    "                         optimizer, device, path_to_store_model_weights, path_to_store_model):\n",
    "\n",
    "    # Step 1: Transform image to tensor\n",
    "    transformation = transforms.ToTensor()\n",
    "\n",
    "    # Step 2: Load the training-, validation- and test dataset with ImageFolder and extract the corresponding labels\n",
    "    training_dataset = datasets.ImageFolder(path_to_training_images, transformation)\n",
    "    training_labels = [item[1] for item in training_dataset.imgs]\n",
    "\n",
    "    val_dataset = datasets.ImageFolder(path_to_validation_images, transformation)\n",
    "    val_labels = [item[1] for item in val_dataset.imgs]\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(path_to_test_images, transformation)\n",
    "    test_labels = [item[1] for item in test_dataset.imgs]\n",
    "\n",
    "    # Step 3: Normalize distance and define, mining_function, thresholdReducer and loss_function\n",
    "    distance = distances.LpDistance(normalize_embeddings=True, p=2, power=1)\n",
    "    mining_func = miners.TripletMarginMiner(margin=0.2, type_of_triplets=\"semihard\", distance=distance)\n",
    "    reducer = reducers.ThresholdReducer(low=0)\n",
    "    loss_fn = losses.TripletMarginLoss(margin=0.2, distance=distance, reducer=reducer, swap=False, triplets_per_anchor=\"all\", smooth_loss=False)\n",
    "\n",
    "    # Step 4: Load training-, validation- and testdataset with DataLoader and sampler\n",
    "    train_loader = DataLoader(training_dataset, batch_size=batch_size_training, sampler = samplers.MPerClassSampler(labels=training_labels, m=20, length_before_new_iter=180000))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size_validation, sampler = samplers.MPerClassSampler(labels=val_labels, m=20, length_before_new_iter=100000))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size_validation, sampler = samplers.MPerClassSampler(labels=test_labels, m=20, length_before_new_iter=100000))\n",
    "\n",
    "    # Step 5: Start training loop\n",
    "    train_tripplet(siamese_model, train_loader, val_loader, optimizer, mining_func, loss_fn, epoch_number, threshold, device, path_to_store_model_weights, path_to_store_model)\n",
    "\n",
    "    # Step 6: Make actual predictions\n",
    "    evaluation_df = prediction_tripplet(siamese_model, test_loader, device, threshold, loss_fn)\n",
    "\n",
    "    # Step 7: Store evaluation_df in a csv file\n",
    "    evaluation_df.to_csv(\"tripplet_evaluation_df\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c7d36",
   "metadata": {
    "id": "b66c7d36"
   },
   "source": [
    "## Step 4: Define main function --> Run the main function to run the program!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce0331ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce0331ed",
    "outputId": "3d465dd1-a22e-4d02-da77-4093d0418cc1"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m         sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 68\u001b[0m   main()\n",
      "Cell \u001b[1;32mIn[21], line 60\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m     run_contrastive(path_to_training_images, path_to_validation_images, path_to_test_images, siamese_model,\n\u001b[0;32m     57\u001b[0m                      epoch_number, batch_size_training, batch_size_validation, learning_rate, threshold,\n\u001b[0;32m     58\u001b[0m                      optimizer, device, path_to_store_weights, path_to_store_model)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m(loss_function \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTripplet\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 60\u001b[0m     run_tripplet(path_to_training_images, path_to_validation_images, path_to_test_images, siamese_model,\n\u001b[0;32m     61\u001b[0m                      epoch_number, batch_size_training, batch_size_validation, learning_rate, threshold,\n\u001b[0;32m     62\u001b[0m                      optimizer, device, path_to_store_weights, path_to_store_model)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe defined loss function is not implemented - Please use Contrastive or Tripplet Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m, in \u001b[0;36mrun_tripplet\u001b[1;34m(path_to_training_images, path_to_validation_images, path_to_test_images, siamese_model, epoch_number, batch_size_training, batch_size_validation, learning_rate, threshold, optimizer, device, path_to_store_model_weights, path_to_store_model)\u001b[0m\n\u001b[0;32m      6\u001b[0m transformation \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Step 2: Load the training-, validation- and test dataset with ImageFolder and extract the corresponding labels\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m training_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(path_to_training_images, transformation)\n\u001b[0;32m     10\u001b[0m training_labels \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m training_dataset\u001b[38;5;241m.\u001b[39mimgs]\n\u001b[0;32m     12\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(path_to_validation_images, transformation)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    310\u001b[0m         root,\n\u001b[0;32m    311\u001b[0m         loader,\n\u001b[0;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[38;5;28;01mif\u001b[39;00m is_valid_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    313\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[0;32m    314\u001b[0m         target_transform\u001b[38;5;241m=\u001b[39mtarget_transform,\n\u001b[0;32m    315\u001b[0m         is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file,\n\u001b[0;32m    316\u001b[0m     )\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m    144\u001b[0m classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m--> 145\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:189\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m make_dataset(directory, class_to_idx, extensions\u001b[38;5;241m=\u001b[39mextensions, is_valid_file\u001b[38;5;241m=\u001b[39mis_valid_file)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:85\u001b[0m, in \u001b[0;36mmake_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m     83\u001b[0m class_index \u001b[38;5;241m=\u001b[39m class_to_idx[target_class]\n\u001b[0;32m     84\u001b[0m target_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, target_class)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(target_dir):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, _, fnames \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mwalk(target_dir, followlinks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n",
      "File \u001b[1;32m<frozen genericpath>:42\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### THE ACTUAL PROGRAM STARTS FROM HERE! ###\n",
    "\n",
    "'''\n",
    "   HYPERPARAMETERS USED FOR CONTRASTIVE_LOSS:\n",
    "    1. epoch_number = 10\n",
    "    2. batch_size_training = 32\n",
    "    3. batch_size_validation = 40\n",
    "    4. learning_rate = 0.001\n",
    "    5. treshold = 0.64\n",
    "\n",
    "   HYPERPARAMETERS USED FOR Triplet Loss:\n",
    "    1. epoch_number = 10\n",
    "    2. batch_size_training = 1000 --> A large batch size of 1000 was chosen to ensure enough \"semihard\" triplets can be found within each batch during the online triplet mining.\n",
    "    3. batch_size_validation = 40 --> The batch size 40 was chosen to ensure that only two identities are loaded into one batch during validation. All possible combinations \n",
    "                                        between images are computed during validation, and random combinations of images are provided by the sampler.\n",
    "    4. learning_rate = 0.005\n",
    "    5. treshold = 0.0625\n",
    "    5. TRIPLET LOSS: SAMPLER      --> The MPerClassSampler retrieves all 20 images of one identity into a batch. This is to ensure that enough positives are available to train the model.\n",
    "                                        Setting length_before_new_iter to the number of images in the Dataset (180000) determines the lenght of the Dataloader (180000/1000) = 180. Later, this is the \n",
    "                                        number of iterations per epoch. \n",
    "\n",
    "    \n",
    "    COMMENT ON TRESHOLDS: Optimal thresholds were chosen by running the evaluation_df trough a ROC curve, which is done in another notebook. \n",
    "    COMMENT ON SIMILARITY PROBABILITY: To get a similarity probability of two embeddings, the Cosine Similarity or Euclidean distance needs to be converted. This is also done in the other notebook.\n",
    "                                        \n",
    "                                        '''\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Step 1: Define paths to the training-, validation- and test set and where the weights and models should be stored\n",
    "    path_to_training_images = 'dataset/unzipped/generated_images_10Kids_cropped/training'\n",
    "    path_to_validation_images = 'dataset/unzipped/generated_images_10Kids_cropped/validation'\n",
    "    path_to_test_images = 'dataset/unzipped/generated_images_10Kids_cropped/testing'\n",
    "    path_to_store_weights = 'dataset'\n",
    "    path_to_store_model = '/dataset'\n",
    "\n",
    "    # Step 2: Define device for GPU usage if possible\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Step 3: Create initial CNN model instance\n",
    "    siamese_model = FaceRecognitionModel(embedding_size=128).to(device)\n",
    "\n",
    "    # Step 4: Set Hyperparameters\n",
    "    epoch_number = 10\n",
    "    batch_size_training = 1000\n",
    "    batch_size_validation = 40\n",
    "    learning_rate = 0.005\n",
    "    loss_function = \"Tripplet\" # Choose one of \"Contrastive\" or \"Tripplet\"\n",
    "    threshold = 0.64\n",
    "    optimizer = optim.Adam(siamese_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Step 5: Start the program with the corresponding loss function implementation\n",
    "    if(loss_function == \"Contrastive\"):\n",
    "        run_contrastive(path_to_training_images, path_to_validation_images, path_to_test_images, siamese_model,\n",
    "                         epoch_number, batch_size_training, batch_size_validation, learning_rate, threshold,\n",
    "                         optimizer, device, path_to_store_weights, path_to_store_model)\n",
    "    elif(loss_function == \"Tripplet\"):\n",
    "        run_tripplet(path_to_training_images, path_to_validation_images, path_to_test_images, siamese_model,\n",
    "                         epoch_number, batch_size_training, batch_size_validation, learning_rate, threshold,\n",
    "                         optimizer, device, path_to_store_weights, path_to_store_model)\n",
    "    else:\n",
    "        print(\"The defined loss function is not implemented - Please use Contrastive or Tripplet Loss\")\n",
    "        sys.exit(-1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7962f49",
   "metadata": {},
   "source": [
    "##  ------------------------------------------------------\n",
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONLY FOR ILLUSTRATION PURPOSES! ###\n",
    "# Architecture with ~185.000 Parameters - This class is not used anymore, but for illustration purposes still in this notebook\n",
    "class SiameseNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNeuralNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional layers with BatchNormalization\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        # Fully connected layers with BatchNormalization\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
